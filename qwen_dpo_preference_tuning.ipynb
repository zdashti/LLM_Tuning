{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cf475-ddc6-44bc-a77a-5a1300a272ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate bitsandbytes peft trl evaluate wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486b4625-8266-4141-910c-8a42c2387a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6e8ae29-3765-44c0-bdfc-38f1c8bc457e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['system', 'question', 'chosen', 'rejected'],\n",
       "    num_rows: 900\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_comp = load_dataset(\"Intel/orca_dpo_pairs\")\n",
    "ds_comp\n",
    "subset = ds_comp[\"train\"].select(range(1000))\n",
    "\n",
    "ds = subset.train_test_split(test_size=0.1, seed=42)\n",
    "ds[\"valid\"] = ds.pop(\"test\")\n",
    "\n",
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52621109-cfee-46d5-ad0a-c906c847bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e5c35fc-b638-451a-a4c3-fb463341366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_prompt(user_input, system_message=None, default_system=\"You are a helpful assistant.\"):\n",
    "    parts = []\n",
    "    sys_msg = (system_message or default_system or \"\").strip()\n",
    "    if sys_msg:\n",
    "        parts.append(f\"<|im_start|>system\\n{sys_msg}<|im_end|>\\n\")\n",
    "    parts.append(f\"<|im_start|>user\\n{(user_input or '').strip()}<|im_end|>\\n\")\n",
    "    parts.append(\"<|im_start|>assistant\\n\")  \n",
    "    return \"\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ada2e58-2ddc-4f96-a620-40d2e3e2c386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a916e03a41eb42bcb83c448b8b49f3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd0c288640143e586e27bdf407b8d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['chosen', 'rejected', 'prompt'],\n",
      "        num_rows: 900\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['chosen', 'rejected', 'prompt'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def to_dpo_rows(ex):\n",
    "    prompt = format_chat_prompt(\n",
    "        user_input=ex.get(\"question\", \"\"),\n",
    "        system_message=ex.get(\"system\", None),\n",
    "    )\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": (ex.get(\"chosen\") or \"\").strip(),         \n",
    "        \"rejected\": (ex.get(\"rejected\") or \"\").strip(),      \n",
    "    }\n",
    "\n",
    "split = split.map(to_dpo_rows, remove_columns=[c for c in split[\"train\"].column_names if c not in {\"prompt\",\"chosen\",\"rejected\"}])\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0429d24f-e1a8-4d9c-ab3a-d47cdf424d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_name = model_name.split('/')[1].replace(\"Instruct\", \"DPO\")\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=ft_model_name,\n",
    "    logging_steps=25,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    bf16=True,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d6e09ac-85bc-4ad0-87b5-70a23d335c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d743abbeb4d4e34be07313a8cb59b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffd6093f9e74ed8ae8297762ae19b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2764ac4bf384952a1d1395c6b4f201b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bce8c6bfab4ae2ac069ac0765175a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe157eaeeda64a02bfbf2cddc83994e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3eedef964746db9206979ec8296ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.\n",
      "C:\\Users\\GitiAI\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [339/339 24:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.633890</td>\n",
       "      <td>-0.018608</td>\n",
       "      <td>-0.143236</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.124627</td>\n",
       "      <td>-304.779175</td>\n",
       "      <td>-339.773468</td>\n",
       "      <td>-1.832836</td>\n",
       "      <td>-2.162092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.563502</td>\n",
       "      <td>-0.056189</td>\n",
       "      <td>-0.344709</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.288521</td>\n",
       "      <td>-305.154999</td>\n",
       "      <td>-341.788208</td>\n",
       "      <td>-1.828329</td>\n",
       "      <td>-2.151340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.530544</td>\n",
       "      <td>-0.071664</td>\n",
       "      <td>-0.444420</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.372755</td>\n",
       "      <td>-305.309692</td>\n",
       "      <td>-342.785339</td>\n",
       "      <td>-1.826217</td>\n",
       "      <td>-2.147258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GitiAI\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\GitiAI\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=339, training_loss=0.60055751997461, metrics={'train_runtime': 1469.2139, 'train_samples_per_second': 1.838, 'train_steps_per_second': 0.231, 'total_flos': 0.0, 'train_loss': 0.60055751997461, 'epoch': 3.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=split['train'],\n",
    "    eval_dataset=split['valid'],\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87e678a9-4897-4edb-99a0-241032935b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./qwen_dpo_orca\")\n",
    "ft_model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf9c8594-ff42-47d6-adac-a6322008ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Model Output:\n",
      " system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "\n",
      "assistant\n",
      "I'm here to help! What would you like assistance with?\n",
      "\n",
      "Chosen Reference:\n",
      " 1. South Africans were waiting for the announcement of election results in which the ruling African National Congress (ANC) was expected to win in a landslide, with ANC leader Jacob Zuma being the likely next president.\n",
      "2. With one-third of the ballots counted, South African media reported that the ANC had garnered around 65% of the vote, while the opposition Democratic Alliance seemed poised to take control of Western Cape province.\n",
      "3. Jacob Zuma addressed supporters at ANC headquarters in Johannesburg, cautioning against early celebrations but claiming the party to be like an \"elephant\" that could not be toppled.\n",
      "4. Final election results were anticipated on Saturday, with manual vote counting being the norm in the country.\n",
      "5. The ANC has faced allegations of corruption and accusations of failing to deliver services to the poor, with former corruption charges against Zuma having been dropped two weeks before the elections due to alleged political interference.\n",
      "6. The election saw 26 parties competing, including Islamic and Christian parties, right-wing Afrikaaner, and socialist groups, with more than 5,000 domestic and international observers monitoring the election process.\n",
      "\n",
      "Rejected Reference:\n",
      " Sure, here are some highlights from the article:\n",
      "\n",
      "1. The ruling African National Congress (ANC) is on course to win the election in a landslide, with around 65% of the vote with one-third of ballots counted.\n",
      "2. The ANC is expected to win in all eight provinces, with the opposition Democratic Alliance (DA) likely to gain control of the Western Cape province.\n",
      "3. ANC leader Jacob Zuma is expected to be the country's next president, and he has stressed that the party is \"not yet celebrating victory.\"\n",
      "4. The election has been marred by long lines and a shortage of ballot boxes and papers in some districts and voting stations, according to the Independent Electoral Commission.\n",
      "5. The ANC has been dogged by allegations of corruption and has been accused of failing to deliver services to the poor.\n",
      "6. The decision to drop fraud and corruption charges against Zuma two weeks before the election sparked widespread outrage and accusations of political interference.\n",
      "7. Twenty-six parties vied in Wednesday's election, including Islamic and Christian parties, and right-wing Afrikaaner and socialist groups.\n",
      "8. More than 5,000 domestic and international observers are monitoring the election, according to the electoral commission.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"./qwen_dpo_orca\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "prompt = split[\"valid\"][0][\"prompt\"]\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"Prompt:\\n\", prompt)\n",
    "print(\"\\nModel Output:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"\\nChosen Reference:\\n\", example[\"chosen\"])\n",
    "print(\"\\nRejected Reference:\\n\", example[\"rejected\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49393535-d1a1-4a70-9f36-def4fbfb79b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5305436849594116, 'eval_runtime': 20.4304, 'eval_samples_per_second': 4.895, 'eval_steps_per_second': 2.447, 'eval_rewards/chosen': -0.07166408747434616, 'eval_rewards/rejected': -0.4444195032119751, 'eval_rewards/accuracies': 0.949999988079071, 'eval_rewards/margins': 0.3727554380893707, 'eval_logps/chosen': -305.3096923828125, 'eval_logps/rejected': -342.78533935546875, 'eval_logits/chosen': -1.8262170553207397, 'eval_logits/rejected': -2.1472580432891846, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c460070-cc58-4356-852c-72dffbdf60a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference accuracy on validation: 16.00%\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "def score_prompt(model, tokenizer, prompt, response):\n",
    "    text = prompt + response\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss.item()\n",
    "    return -loss  # امتیاز بالاتر بهتر است\n",
    "\n",
    "scores = []\n",
    "for ex in split[\"valid\"].select(range(50)):  # روی ۵۰ نمونه تست می‌کنیم\n",
    "    s_chosen = score_prompt(model, tokenizer, ex[\"prompt\"], ex[\"chosen\"])\n",
    "    s_rejected = score_prompt(model, tokenizer, ex[\"prompt\"], ex[\"rejected\"])\n",
    "    scores.append(s_chosen > s_rejected)\n",
    "\n",
    "accuracy = np.mean(scores)\n",
    "print(f\"Preference accuracy on validation: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7a7b6-925f-474d-83bc-61ccf4a9d8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
